% ============================================================================
% DEEP ANALYSIS SECTIONS FOR PAPER
% ============================================================================
% These sections provide deeper rationale and connect decisions to research questions

\section{Design Rationale and Research Question Alignment}

Our experimental design directly addresses two questions: (i) \emph{when} do policy-gradient methods improve over SFT, and (ii) \emph{which} reward designs avoid failure modes. Here we justify each major decision and connect it to these questions.

\subsection{Why Small Models?}

We use \model{} (80M parameters) rather than larger variants. This choice is central to answering ``when'' because \emph{stability becomes the limiting factor} in small-model regimes. Large models can mask RL instability through capacity; small models force us to address stability directly. If RL helps small models, it's a stronger signal than if it only helps with massive capacity. Our results (SCST from SFT: \rougel{}~0.1852, +1.8\% over SFT) demonstrate that RL can help even with limited capacity when properly stabilized, directly answering ``when'' in the most constrained setting.

\subsection{Why Warm-Start from SFT?}

Both SCST and PPO experiments warm-start from SFT rather than the base model. This addresses ``when'' directly: \emph{RL improves after SFT establishes task competence, not as a replacement for it}. 

Theoretical justification: Policy gradient methods require the policy to assign non-trivial probability to good actions. A base model may assign near-zero probability to coherent summaries, making gradients uninformative. SFT provides a competent initial policy (e.g., $P(\text{good summary} \mid \text{article}) \approx 0.1$--0.3), enabling RL to shift probability mass toward better summaries.

Empirical evidence: SCST from SFT achieves \rougel{}~0.1852 vs.\ SCST from base 0.1389---a gap of +0.0463 (33\% relative). This quantifies the importance of warm-starting and answers ``when'': RL helps \emph{when} initialized from a competent policy.

\subsection{Why ROUGE-L as Primary Reward?}

We use \rougel{} as the primary reward signal, addressing ``which reward designs avoid failure modes.'' \rougel{} measures longest common subsequence, capturing semantic overlap while being less sensitive to length mismatches than \rougel{}-1/2 (which can reward verbosity). However, \rougel{} alone doesn't directly penalize repetition, motivating our mixed reward design (Eq.~\ref{eq:reward}) with small repetition and length terms.

Our SCST results with pure \rougel{} show compression 2.36 (good) and repetition 0.28 (moderate), suggesting \rougel{} + decoding constraints mostly avoids both failure modes. The mixed reward (pending PPO runs) will test whether explicit penalties provide better control.

\subsection{Why Minimalist Mixed Reward?}

Equation~\ref{eq:reward} combines \rougel{} with small structural terms ($\gamma$ for repetition, $\delta$ for length). We keep coefficients small to avoid conflicting signals and maintain interpretability. Large $\gamma$ can collapse generation to very short outputs; large $\delta$ can cause over-shortening. Small coefficients provide gentle nudges without overriding the primary \rougel{} signal.

This design directly tests Q2: can a minimalist approach avoid both verbosity and loops? Our SCST results suggest yes for the warm-started case; PPO with mixed rewards will test explicit control.

\subsection{Why SCST Before PPO?}

We prioritize SCST for initial experiments due to simplicity (no value head, fewer hyperparameters) and theoretical elegance (self-critical baseline naturally reduces variance). This allows us to answer ``when'' for the simplest case first. PPO provides more control (KL constraint, value head) but adds complexity; comparing both tests whether additional control helps or hurts in small-model regimes.

\section{Hyperparameter Choices: Stability Over Speed}

All hyperparameters prioritize stability, which is necessary for RL to help small models (addressing Q1's ``when'').

\paragraph{Learning rates.}
SFT uses $5 \times 10^{-5}$; RL uses $10^{-6}$ (5--10$\times$ lower). Policy gradients have higher variance than supervised gradients; lower rates prevent instability. Our PPO verification showed weight changes $\sim 10^{-5}$, confirming learning without collapse.

\paragraph{Decoding constraints.}
\texttt{no\_repeat\_ngram\_size=4}, \texttt{top\_p=0.75--0.80}. The no-repeat constraint directly prevents loops (Q2 failure mode). We chose 4-grams: 3 is too strict (blocks legitimate phrases like ``the United States''), 5+ allows longer repetitive sequences. Nucleus sampling (top-$p$) is more adaptive than top-$k$ to distribution shape. The 0.75--0.80 range balances diversity (lower values reduce it too much) and stability (higher values increase variance).

\paragraph{Data filtering.}
We keep articles with 200--1200 characters (64--2000 for PPO). Very short articles produce trivial summaries; very long ones have high reward variance. Filtering reduces variance, enabling stable training. This is part of answering ``when'': RL helps when variance is controlled through data curation.

\paragraph{Reward clamping.}
We clamp per-example rewards to $[-0.5, 0.5]$. A single outlier reward can destabilize training; smaller ranges produce more conservative updates. Symmetric clamping ensures negative advantages (when sampled $<$ greedy) are handled correctly. In pilot runs without clamping, reward spikes caused policy collapse; with clamping, training is stable.

\section{Results Interpretation: Direct Answers to Research Questions}

\subsection{Answering Q1: When Do Policy-Gradient Methods Improve?}

\textbf{Direct answer}: RL improves over SFT \emph{when}:
\begin{enumerate}
\item Warm-started from SFT (not base model)
\item Stability mechanisms are in place (decoding constraints, reward clamping, data filtering)
\item Sufficient training data (2000+ examples for SCST)
\item Proper implementation (T5 generation fix, DataLoader handling)
\end{enumerate}

\textbf{Evidence}: SCST from SFT achieves \rougel{}~0.1852 vs.\ SFT 0.1820 (+0.0032, +1.8\% relative). SCST from base achieves only 0.1389, demonstrating warm-starting is necessary.

\textbf{Theoretical explanation}: Policy gradients require non-trivial probability on good actions. Base models: $P(\text{good} \mid \text{article}) \approx 0$ → uninformative gradients. SFT models: $P(\text{good} \mid \text{article}) \approx 0.1$--0.3 → informative gradients that RL can exploit.

\subsection{Answering Q2: Which Reward Designs Avoid Failure Modes?}

\textbf{Direct answer}: 
\begin{itemize}
\item \rougel{} + decoding constraints mostly avoids both verbosity and loops (SCST from SFT: compression 2.36, repetition 0.28)
\item Mixed reward (designed, pending PPO) should provide more explicit control
\item \textbf{Key insight}: Decoding constraints are as important as reward design
\end{itemize}

\textbf{Evidence}: 
\begin{itemize}
\item SCST(SFT) with \rougel{} only: compression 2.36 (good), repetition 0.28 (moderate) → mostly avoids both
\item SCST(Base) with \rougel{} only: compression 0.79 (bad---verbosity), repetition 0.11 (good) → shows \rougel{} alone isn't enough without warm-starting
\end{itemize}

\textbf{Design rationale}: The mixed reward (Eq.~\ref{eq:reward}) explicitly addresses both failure modes:
\begin{itemize}
\item $-\gamma \,\mathrm{rep}(y)$: Directly penalizes loops
\item $-\delta \max(0, \mathrm{comp}(y,\hat{y}) - c^\star)$: Penalizes verbosity when compression exceeds target
\end{itemize}
Small coefficients provide gentle nudges without overriding the primary \rougel{} signal.

\section{Engineering Decisions: Why They Matter}

\subsection{T5 Generation Fix}

The T5 generation extraction bug (Sec.~6) is \emph{necessary but not sufficient} for RL to help. Without correct extraction, generations are empty and RL cannot run, preventing us from answering Q1. This fix is a requirement for any RL work on encoder--decoder models, not just a bug fix.

\subsection{SCST DataLoader Fix}

The \texttt{DictDataset} wrapper (Sec.~A) enables SCST to run by properly handling dictionary batches. This is foundational---we cannot answer research questions if experiments don't run. Our design choice (wrapper vs.\ pre-tokenization) keeps data in natural format until generation, maintaining flexibility.

\subsection{Advantage Normalization in SCST}

We normalize advantages (mean=0, std=1) in SCST to reduce variance. Policy gradient variance scales with advantage variance; normalization makes gradients more reliable. PPO uses a value head for this purpose; SCST relies on normalization. This is another stability mechanism that enables RL to help (Q1).

\section{Trade-offs and Design Philosophy}

\subsection{Stability vs.\ Performance}

We prioritize stability over maximum performance. Small models are fragile; instability can cause complete training failure. Conservative hyperparameters, strict decoding constraints, and reward clamping produce slower but more reliable learning. This philosophy directly answers ``when'': RL helps when stability is prioritized, even if it means slower convergence.

\subsection{Simplicity vs.\ Sophistication}

We start simple and add complexity only when needed. SCST uses pure \rougel{} before testing mixed rewards; we warm-start from SFT before testing curriculum learning. This approach establishes baselines and maintains interpretability. It answers Q1 for the simplest case first, then allows testing whether sophistication helps.

\subsection{Engineering vs.\ Theory}

We document engineering details that materially affect results. The T5 fix, DataLoader handling, and reward clamping are not theoretically interesting but are practically critical. These choices are part of the ``when'' and ``which'' answers: RL helps when these are handled correctly, and reward designs work when properly implemented.

\section{Limitations and Their Implications}

\paragraph{Small models limit absolute performance.}
Our best \rougel{}~0.1852 is far from SOTA ($\sim$0.40+). However, we ask ``when does RL help?'' not ``can RL reach SOTA?'' Our contribution is about \emph{relative improvement} and \emph{failure mode avoidance}, not absolute performance.

\paragraph{Single epoch for SCST.}
We use 1 epoch to show proof-of-concept within compute budgets. This answers ``when does RL help?'' with ``after 1 epoch from SFT,'' not claiming maximum possible improvement. Multi-epoch training might yield larger gains.

\paragraph{Fixed evaluation slice.}
We evaluate on a fixed 1k validation set for fair comparison and reproducibility. This may not capture full dataset diversity but ensures fair comparison when answering ``when'' and ``which designs.''

\section{Broader Implications}

\paragraph{For small-model RL.}
Our work shows RL is feasible for small models with proper engineering. RL isn't just for large models; it can help in resource-constrained settings, but requires careful stability engineering (our Sec.~6).

\paragraph{For reward design.}
We show minimalist rewards can work; you don't need complex designs. Start simple, add complexity only when needed. This answers Q2 by showing simple designs can avoid failure modes.

\paragraph{For reproducibility.}
We document engineering details often omitted in RL papers. Others can reproduce our work and build on it, though some details are dataset/model-specific (T5 fix, \cnndm{} filtering).

\section{Open Questions and Future Work}

\paragraph{Remaining questions for Q1.}
\begin{itemize}
\item Does multi-epoch RL yield larger gains?
\item Is PPO better than SCST for small models? (PPO pending)
\item How sensitive are results to reward coefficients?
\item Does RL help more with more training data?
\end{itemize}

\paragraph{Remaining questions for Q2.}
\begin{itemize}
\item What are optimal coefficients for mixed reward?
\item How does length target $c^\star$ affect compression and quality?
\item What's the optimal repetition penalty strength?
\end{itemize}

These show our work is a \emph{foundation}, not a complete answer. We establish that RL can help (Q1) and simple designs can avoid failure modes (Q2), but there's room for optimization.

